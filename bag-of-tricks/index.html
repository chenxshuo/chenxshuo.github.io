<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <script src="https://tsb0601.github.io/mmvp_blog/template.v2.js"></script> <script src="https://tsb0601.github.io/mmvp_blog/contents_bar.js"></script> <script src="https://d3js.org/d3.v5.min.js"></script> <script src="https://d3js.org/d3-collection.v1.min.js"></script> <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script> <script src="https://tsb0601.github.io/mmvp_blog/cross_fade.js"></script> <link rel="stylesheet" href="../assets/css/micl-colm-style.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>Bag of Tricks for Subverting Reasoning-based Safety Guardrails</title> </head> <body> <div class="header-container"> <div class="header-content"> <h1>Bag of Tricks for Subverting Reasoning-based Safety Guardrails</h1> <p> üèÜ Honorable Mention Award in Red‚ÄëTeaming Challenge - OpenAI on Kaggle </p> <div class="button-container"> <a href="https://arxiv.org/abs/2510.11570" class="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/writeups/jailbreaking-deliberative-alignment-via-structural" class="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/writeups/jailbreaking-deliberative-alignment-via-structural" class="button" rel="external nofollow noopener" target="_blank">Kaggle Writeup</a> <a href="https://docs.google.com/presentation/d/1OqgGAj7f5YCGw0bdzFqEaZQuijHmbZ-WS3-9M0lN8Ds/edit?usp=sharing" class="button" rel="external nofollow noopener" target="_blank">Oral Presentation Slides</a> </div> </div> <div class="header-image"> <img src="https://plus.unsplash.com/premium_photo-1668365370581-418a8cc7df1a?q=80&amp;w=880&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.1.0&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" alt="" class="teaser-image" style="max-width: 60%; height: 60%; display: block; margin: 0 auto 20px; border-radius: 8px;"> </div> </div> <d-article> <p> Recent reasoning-based safety guardrails for Large Reasoning Models (LRMs), such as deliberative alignment, have shown strong defense against jailbreak attacks. By leveraging LRMs‚Äô reasoning ability, these guardrails help the models to assess the safety of user inputs before generating final responses. The powerful reasoning ability can analyze the intention of the input query and will refuse to assist once it detects the harmful intent hidden by the jailbreak methods. Such guardrails have shown a significant boost in defense, such as the near-perfect refusal rates on the open-source gpt-oss series. Unfortunately, we find that these powerful reasoning-based guardrails can be extremely vulnerable to subtle manipulation of the input prompts, and once hijacked, can lead to even more harmful results. Specifically, we first uncover a surprisingly fragile aspect of these guardrails: simply adding a few template tokens to the input prompt can successfully bypass the seemingly powerful guardrails and lead to explicit and harmful responses. To explore further, we introduce a bag of jailbreak methods that subvert the reasoning-based guardrails. Our attacks span white-, gray-, and black-box settings and range from effortless template manipulations to fully automated optimization. Along with the potential for scalable implementation, these methods also achieve alarmingly high attack success rates (e.g., exceeding 90% across 5 different benchmarks on gpt-oss series on both local host models and online API services). Evaluations across various leading open-source LRMs confirm that these vulnerabilities are systemic, underscoring the urgent need for stronger alignment techniques for open-sourced LRMs to prevent malicious misuse. </p> </d-article> </body> </html>