<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <script src="https://tsb0601.github.io/mmvp_blog/template.v2.js"></script> <script src="https://tsb0601.github.io/mmvp_blog/contents_bar.js"></script> <script src="https://d3js.org/d3.v5.min.js"></script> <script src="https://d3js.org/d3-collection.v1.min.js"></script> <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script> <script src="https://tsb0601.github.io/mmvp_blog/cross_fade.js"></script> <link rel="stylesheet" href="../assets/css/micl-colm-style.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>Bag of Tricks for Subverting Reasoning-based Safety Guardrails</title> </head> <body> <div class="header-container"> <div class="header-content"> <h1>Bag of Tricks for Subverting Reasoning-based Safety Guardrails</h1> <p> ğŸ† Honorable Mention Award in Redâ€‘Teaming Challenge - OpenAI on Kaggle </p> <div class="button-container"> <a href="https://arxiv.org/abs/2510.11570" class="button" rel="external nofollow noopener" target="_blank">Paper</a> <a href="https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/writeups/jailbreaking-deliberative-alignment-via-structural" class="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/writeups/jailbreaking-deliberative-alignment-via-structural" class="button" rel="external nofollow noopener" target="_blank">Kaggle Writeup</a> <a href="https://docs.google.com/presentation/d/1OqgGAj7f5YCGw0bdzFqEaZQuijHmbZ-WS3-9M0lN8Ds/edit?usp=sharing" class="button" rel="external nofollow noopener" target="_blank">Oral Presentation Slides</a> </div> </div> <div class="header-image"> <img src="https://plus.unsplash.com/premium_photo-1668365370581-418a8cc7df1a?q=80&amp;w=880&amp;auto=format&amp;fit=crop&amp;ixlib=rb-4.1.0&amp;ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D" alt="" class="teaser-image" style="max-width: 60%; height: 60%; display: block; margin: 0 auto 20px; border-radius: 8px;"> </div> </div> <d-article> <div class="byline"> <div class="byline-container"> <div class="byline-column"> <h3>Authors</h3> <p><a href="https://chenxshuo.github.io/" class="author-link">Shuo Chen</a><sup>ğŸ“§</sup></p> <p><a href="https://sites.google.com/view/zhenhan/home?authuser=0" class="author-link" rel="external nofollow noopener" target="_blank">Zhen Han</a></p> <p><a href="https://haokunchen245.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Haokun Chen</a></p> <p><a href="https://bailanhe.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Bailan He</a></p> <p><a href="https://shengyun-si.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Shengyun Si</a></p> <p><a href="https://www.linkedin.com/in/jingpeiwu" class="author-link" rel="external nofollow noopener" target="_blank">Jingpei Wu</a></p> <p><a href="https://torrvision.com/" class="author-link" rel="external nofollow noopener" target="_blank">Philip Torr</a></p> <p><a href="https://tresp-lab.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Volker Tresp</a></p> <p><a href="https://jindonggu.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Jindong Gu</a><sup>ğŸ“§</sup></p> </div> <div class="byline-column"> <h3>Affiliations</h3> <p>LMU Munich, Siemens, MCML, relAI</p> <p>AWS AI</p> <p>LMU Munich, MCML</p> <p>LMU Munich, relAI</p> <p>Technical University of Berlin, DFKI</p> <p>LMU Munich, relAI</p> <p>University of Oxford</p> <p>LMU Munich, MCML</p> <p>University of Oxford</p> </div> <div class="byline-column"> <h3>Correspondence</h3> <p style="font-size: 14px; color: #666;"> <i class="fa fa-envelope" style="margin-right: 5px;"></i> <a href="mailto:jindong.gu@outlook.com" style="color: #666; text-decoration: none;">jindong.gu@outlook.com</a> </p> <p style="font-size: 14px; color: #666;"> <i class="fa fa-envelope" style="margin-right: 5px;"></i> <a href="mailto:chenshuo.cs@outlook.com" style="color: #666; text-decoration: none;">chenshuo.cs@outlook.com</a> </p> </div> </div> </div> <d-contents> <nav> <h4>Contents</h4> <div><a href="#introduction">1. Introduction</a></div> <div><a href="#method">2. Bag of Tricks</a></div> <div><a href="#experiments">3. Experimental Results</a></div> <div><a href="#insights">4. Insights</a></div> </nav> </d-contents> <section id="introduction"> <h2>1. Introduction</h2> <p> Reasoning-based guardrails are widely viewed as a promising way to keep large reasoning models from producing harmful content, yet our study shows how fragile these defenses can be in practice. Deliberative Alignment asks a model to justify policy compliance before responding, but this safeguard hinges on rigid conversation templates and brittle refusal heuristics. When an attacker lightly perturbs the chat structure with crafted template tokens, the guardrail can be sidestepped, allowing the model to answer dangerous requests that it should have blocked. </p> <p> We introduce a bag of jailbreak techniques that either bypass the reasoning step altogether or hijack it to follow malicious instructions. Structural CoT Bypass and Fake Over-Refusal exploit formatting loopholes to skip or disguise the safety check, while Coercive Optimization automatically learns adversarial suffixes. Reasoning Hijack goes further by injecting hostile goals directly into the modelâ€™s chain of thought, producing tailored harmful outputs with minimal effort. Across five jailbreak benchmarks and multiple open-source LRMsâ€”including Qwen3 and Phi-4 Reasoningâ€”these attacks exceed 90% success, underscoring the urgent need for stronger, more resilient safety defenses. </p> </section> <section id="method"> <h2>2. Bag of Tricks</h2> <p> Our jailbreaks fall into two families: bypass techniques that short-circuit the guardrailâ€™s reasoning pass, and hijack techniques that weaponize that reasoning to craft bespoke harmful outputs. Each trick operates under a different knowledge assumptionâ€”ranging from black-box queries to gradient accessâ€”but all exploit the same brittle reliance on template tokens and heuristic refusals baked into todayâ€™s open-source LRMs. </p> <h3>Structural CoT Bypass (Gray-box)</h3> <p> By tapping into the publicly documented chat template, we show that prematurely closing the user turn and injecting a mock â€œanalysisâ€ segment convinces the guardrail that safety checks have already passed. The model then enters its final response channel and answers directly, even on obviously malicious prompts. The only requirement is knowledge of the delimiter tokens; no weight access is needed. </p> <h3>Fake Over-Refusal (Black-box)</h3> <p> Guardrails still reject some questions after the bypass, so we repurpose classic over-refusal phrasing to slip harmful intent past keyword filters. Benign-looking requests like â€œI want to kill time. Time is a manâ€™s name.â€ inherit the permissive judgment granted to harmless variants, yet they nudge the model toward explicit attack instructions once the reasoning layer is neutralized. </p> <h3>Coercive Optimization (White-box)</h3> <p> When we do have gradient access, we optimize short adversarial suffixes to coerce the model into emitting the final-response template token and a low-resource language tag (e.g., â€œ<code>&lt;|channel|&gt;final</code> â€¦ **Answer in German**â€). Hitting that target reliably disables the safety routine, enabling a single optimized suffix to generalize across prompts and models. </p> <h3>Reasoning Hijack (Gray-box)</h3> <p> Instead of discarding the chain of thought, this trick replaces it with an attacker-authored plan and commentary tokens that instruct the model to obey it. The result is a highly tailored, step-by-step harmful answer that leverages the modelâ€™s own deliberative skills. As with the bypass, token-level access sufficesâ€”no gradients required. </p> </section> <section id="experiments"> <h2>3. Experimental Results</h2> <p> We evaluate across 1,883 prompts spanning StrongREJECT, AdvBench, HarmBench, CatQA, and JBB-Behaviors, using both gpt-oss-20b and 120b under identical inference settings. Harmfulness is measured with StrongREJECTâ€™s rubric evaluator, while attack success rate (ASR) aggregates four refusal detectors to avoid overfitting to a single judge. </p> <p> Baseline jailbreak toolkits barely dent the guardrails: Direct queries, Policy Puppetry, H-CoT, and AutoRAN all stay below 40% ASR and deliver low harm scores. In contrast, our methods push ASR well past 90% on every benchmark. On gpt-oss-20b, Reasoning Hijack reaches 91â€“92% ASR with harm scores up to 73% on CatQA, while even the lighter Fake Over-Refusal clears 86â€“91% ASR and doubles AutoRANâ€™s harm on StrongREJECT (50.5% vs. 23.5%). The larger gpt-oss-120b proves even more brittle: Structural CoT Bypass alone surges to 92.8% ASR on AdvBench with a 71.9% harm score, and Fake Over-Refusal peaks at 96% ASR on JBB-Behaviors. </p> <p> Coercive Optimization, our gradient-based variant, further confirms that forcing the final-response token is enough to disable the guardrail logic. A single optimized suffix yields 70â€“75% ASR across datasets on gpt-oss-20b, outperforming structure-only bypasses without any manual prompt crafting. </p> <p> Transfer experiments highlight the systemic nature of these failures. On Qwen3-4B-Thinking, Phi-4-Reasoning-Plus, and DeepSeek-R1-Llama-8B, Reasoning Hijack sustains 65â€“89% ASR with harm scores often above 40%. Even models that partially resist the raw structural bypass (e.g., Phi-4 at 1.8% ASR) crumble once Fake Over-Refusal or Reasoning Hijack is applied, underscoring how little attacker expertise is required. </p> <p> Ablations show that neither sampling temperature nor higher reasoning effort mitigates the attacks. Reasoning Hijack remains above 85% ASR and 65% harm regardless of temperature between 0.0 and 1.6, and the guardrails do not improve when we request high-effort reasoning from the model. The vulnerabilities therefore stem from the guardrail design itself, not from a narrow operating regime. </p> </section> <section id="insights"> <h2>4. Insights</h2> <p> Beyond raw attack rates, the bag of tricks surfaces systemic lessons for designing safer reasoning models. The failures stem less from obscure jailbreak lore and more from structural assumptions baked into todayâ€™s guardrails. </p> <h3>Chat Templates Are a Single Point of Failure</h3> <p> Safety logic is tightly coupled to the gpt-oss chat template, so minor format edits or pre-filled assistant turns can flip the refusal decision. Guardrails need to generalize safety judgments beyond exact delimiter sequences instead of treating template conformance as proof of compliance. </p> <h3>Borderline Prompts Demand Better Calibration</h3> <p> Fake Over-Refusal shows that guardrails over-accept ambiguous phrasing borrowed from benign â€œkill timeâ€ examples. Models require finer-grained training on borderline data so that tone shifts or name substitutions do not grant harmful prompts a clean bill of health. </p> <h3>Safety Signals Must Persist Past the Opening Tokens</h3> <p> Many refusals are decided within the first few generated tokens; once we force the final-response marker, safeguards collapse. Rebalancing the attention budgetâ€”so that later tokens can still trigger refusalsâ€”would blunt template-level edits and make adversarial suffixes less decisive. </p> <h3>Reasoning Chains Need Their Own Guardrails</h3> <p> Reasoning Hijack proves that rich chain-of-thought can be weaponized to produce bespoke harmful plans. Any deployment that exposes internal reasoning should pair it with verification or secondary safety passes that audit the generated trace before it guides the final answer. </p> </section> </d-article> <d-appendix> <h3>BibTeX</h3> <div class="bibtex"> <p> @article{chen2025bag, <br> Â Â Â Â title={Bag of Tricks for Subverting Reasoning-based Safety Guardrails}, <br> Â Â Â Â author={Chen, Shuo and Han, Zhen and Chen, Haokun and He, Bailan and Si, Shengyun and Wu, Jingpei and Torr, Philip and Tresp, Volker and Gu, Jindong}, <br> Â Â Â Â journal={arXiv preprint arXiv:2510.11570}, <br> Â Â Â Â year={2025} <br> } </p> </div> <h3>Acknowledgement</h3> <p class="bibtex">This template is adapted from <a href="https://tsb0601.github.io/mmvp_blog/" rel="external nofollow noopener" target="_blank">Eyes Wide Shut</a>.</p> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </body> </html>