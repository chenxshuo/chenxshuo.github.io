<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="description" content=""> <meta name="keywords" content="VQD"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>Can Multimodal Large Language Models Truly Perform Multimodal In-Context Learning?</title> <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> <link rel="stylesheet" href="/assets/stop-reasoning/static/css/bulma.min.css"> <link rel="stylesheet" href="/assets/stop-reasoning/static/css/bulma-carousel.min.css"> <link rel="stylesheet" href="/assets/stop-reasoning/static/css/bulma-slider.min.css"> <link rel="stylesheet" href="/assets/stop-reasoning/static/css/fontawesome.all.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"> <link rel="stylesheet" href="/assets/stop-reasoning/static/css/index.css"> <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> <script defer src="/assets/stop-reasoning/static/js/fontawesome.all.min.js"></script> <script src="/assets/stop-reasoning/static/js/bulma-carousel.min.js"></script> <script src="/assets/stop-reasoning/static/js/bulma-slider.min.js"></script> </head> <body> <section class="hero"> <div class="hero-body"> <div class="container is-max-desktop"> <div class="columns is-centered"> <div class="column has-text-centered"> <h1 class="title is-1 publication-title is-bold"> <span class="vqd" style="vertical-align: middle">Can Multimodal Large Language Models Truly Perform Multimodal In-Context Learning?</span> </h1> <div class="is-size-5 publication-authors"> <span class="author-block"> <a href="https://chenxshuo.github.io/">Shuo Chen</a><sup style="color:#6fbf73;">1,</sup><sup style="color:#ed4b82">3</sup>,</span> <span class="author-block"> <a href="https://sites.google.com/view/zhenhan/home" rel="external nofollow noopener" target="_blank">Zhen Han</a><sup style="color:#6fbf73">1</sup>,</span> <span class="author-block"> <a href="https://scholar.google.com/citations?hl=en&amp;user=n5zUQtAAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Bailan He</a><sup style="color:#6fbf73;">1,</sup><sup style="color:#ed4b82">3</sup>, </span> <span class="author-block"> <a href="">Jianzhe Liu</a><sup style="color:#ed4b82">4</sup>, </span><br> <span class="author-block"> <a href="https://www.linkedin.com/in/markbbuckley/?locale=de_DE" rel="external nofollow noopener" target="_blank">Mark Buckley</a><sup style="color:#ed4b82">3</sup>, </span> <span class="author-block"> <a href="https://yaoqin1.github.io/" rel="external nofollow noopener" target="_blank">Yao Qin</a><sup style="color:#eb0a95">5</sup>, </span> <span class="author-block"></span> <a href="https://torrvision.com/index.html" rel="external nofollow noopener" target="_blank">Philip Torr</a><sup style="color:#16e2d4">2</sup>, <span class="author-block"> <a href="https://www.dbs.ifi.lmu.de/~tresp/" rel="external nofollow noopener" target="_blank">Volker Tresp</a><sup style="color:#6fbf73;">1,</sup><sup style="color:#a450db">6</sup>, </span> <span class="author-block"> <a href="https://jindonggu.github.io/" rel="external nofollow noopener" target="_blank">Jindong Gu</a><sup style="color:#16e2d4">2</sup>, </span> </div> <div class="is-size-5 publication-authors"> <span class="author-block"><sup style="color:#6fbf73;">1</sup>LMU Munich,</span> <span class="author-block"><sup style="color:#16e2d4">2</sup>University of Oxford,</span> <span class="author-block"><sup style="color:#ed4b82">3</sup>Siemens AG,</span><br> <span class="author-block"><sup style="color:#ffac33">4</sup>Technical University of Munich</span> <span class="author-block"><sup style="color:#eb0a95">5</sup>University of California, Santa Barbara</span> <span class="author-block"><sup style="color:#a450db">6</sup>Munich Center for Machine Learning (MCML)</span><br> <span class="paper-block"><b style="color:#f41c1c">WACV 2025</b></span> </div> <div class="column has-text-centered"> <div class="publication-links"> <span class="link-block"> <a href="https://arxiv.org/abs/2311.18021" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="fas fa-file-pdf"></i> </span> <span>Paper</span> </a> </span> <span class="link-block"> <a href="https://github.com" class="external-link button is-normal is-rounded is-dark" rel="external nofollow noopener" target="_blank"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span> </div> </div> </div> </div> </div> </div> </section> <section class="section"></section> <div class="container" style="margin-bottom: 2vh;"> <div class="columns is-centered has-text-centered"> <div class="column is-four-fifths"> <h2 class="title is-3">Abstract</h2> <div class="content has-text-justified"> <p> Large Language Models (LLMs) with in-context learning (ICL) ability can quickly adapt to a specific context given a few demonstrations (demos). Recently, Multimodal Large Language Models (MLLMs) built upon LLMs have also shown multimodal ICL ability, \ie, responding to queries given a few multimodal demos, including images, queries, and answers. While ICL has been extensively studied on LLMs, its research on MLLMs remains limited. One essential question is whether these MLLMs can truly conduct multimodal ICL, or if only the textual modality is necessary. We investigate this question by examining two primary factors that influence ICL: 1) Demo content, \ie, understanding the influences of demo content in different modalities. 2) Demo selection strategy, \ie, how to select better multimodal demos for improved performance. Experiments revealed that multimodal ICL is predominantly driven by the textual content whereas the visual information in the demos has little influence. Interestingly, visual content is still necessary and useful for selecting demos to increase performance. Motivated by our analysis, we propose a simple yet effective approach, termed Mixed Modality In-Context Example Selection (MMICES), which considers both visual and language modalities when selecting demos. Extensive experiments are conducted to support our findings and verify the improvement brought by our method. </p> </div> </div> </div> </div> <br> <section class="section" id="BibTeX"> <div class="container is-max-desktop content"> <h2 class="title is-3 has-text-centered">BibTeX</h2> <pre><code>
      @article{chen2023understanding,
        title={Can Multimodal Large Language Models Truly Perform Multimodal In-Context Learning?},
        author={Chen, Shuo and Han, Zhen and He, Bailan and Buckley, Mark and Torr, Philip and Tresp, Volker and Gu, Jindong},
        journal={arXiv preprint arXiv:2311.18021},
        year={2023}
      }
    </code></pre> </div> </section> <footer class="footer"> <div class="content has-text-centered"> </div> <div class="columns is-centered"> <div class="column is-8"> <div class="content"> <p> This website is website adapted from <a href="https://financemath-acl2024.github.io/" rel="external nofollow noopener" target="_blank">FinanceMath</a>. </p> </div> </div> </div> </footer> </body> </html>