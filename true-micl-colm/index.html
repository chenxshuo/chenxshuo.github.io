<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1"> <script src="https://tsb0601.github.io/mmvp_blog/template.v2.js"></script> <script src="https://tsb0601.github.io/mmvp_blog/contents_bar.js"></script> <script src="https://d3js.org/d3.v5.min.js"></script> <script src="https://d3js.org/d3-collection.v1.min.js"></script> <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script> <script src="https://tsb0601.github.io/mmvp_blog/cross_fade.js"></script> <link rel="stylesheet" href="../assets/css/micl-colm-style.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script> <meta charset="UTF-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <title>True Multimodal In-Context Learning Needs Attention to the Visual Context</title> </head> <body> <div class="header-container"> <div class="header-content"> <h1>True Multimodal In-Context Learning Needs Attention to the Visual Context</h1> <div class="button-container"> <a href="#" class="button">Paper (Coming Soon)</a> <a href="https://chenxshuo.github.io/micl-colm/" class="button">Code (Coming Soon)</a> <a href="https://chenxshuo.github.io/micl-colm/" class="button">Dataset (Coming Soon)</a> </div> </div> <div class="header-image"> <img src="/assets/img/micl-colm/micl-gif.gif" alt="DARA Method Animation" class="teaser-image" style="max-width: 100%; height: auto; display: block; margin: 0 auto 20px; border-radius: 8px;"> </div> </div> <d-article> <div class="byline"> <div class="byline-container"> <div class="byline-column"> <h3>Authors</h3> <p><a href="https://chenxshuo.github.io/" class="author-link">Shuo Chen</a>*<sup>ðŸ“§</sup></p> <p><a href="#" class="author-link">Jianzhe Liu</a>*</p> <p><a href="https://sites.google.com/view/zhenhan/home?authuser=0" class="author-link" rel="external nofollow noopener" target="_blank">Zhen Han</a></p> <p><a href="https://yan-xia.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Yan Xia</a></p> <p><a href="https://vision.in.tum.de/members/cremers" class="author-link" rel="external nofollow noopener" target="_blank">Daniel Cremers</a></p> <p><a href="https://torrvision.com/" class="author-link" rel="external nofollow noopener" target="_blank">Philip Torr</a></p> <p><a href="https://tresp-lab.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Volker Tresp</a></p> <p><a href="https://jindonggu.github.io/" class="author-link" rel="external nofollow noopener" target="_blank">Jindong Gu</a><sup>ðŸ“§</sup></p> </div> <div class="byline-column"> <h3>Affiliations</h3> <p>LMU Munich, Siemens AG, MCML, relAI</p> <p>Technical University of Munich</p> <p>LMU Munich</p> <p>University of Science and Technology of China</p> <p>Technical University of Munich</p> <p>University of Oxford</p> <p>LMU Munich, MCML</p> <p>University of Oxford</p> </div> <div class="byline-column"> <h3>Conference</h3> <p> <strong>COLM 2025</strong></p> </div> <div class="byline-column"> <h3>Correspondence</h3> <p style="font-size: 14px; color: #666;"> <i class="fa fa-envelope" style="margin-right: 5px;"></i> <a href="mailto:jindong.gu@outlook.com" style="color: #666; text-decoration: none;">jindong.gu@outlook.com</a> </p> <p style="font-size: 14px; color: #666;"> <i class="fa fa-envelope" style="margin-right: 5px;"></i> <a href="mailto:chenshuo.cs@outlook.com" style="color: #666; text-decoration: none;">chenshuo.cs@outlook.com</a> </p> </div> </div> </div> <d-contents> <nav> <h4>Contents</h4> <div><a href="#introduction">MICL and Visual Context Neglect</a></div> <div><a href="#method">DARA: Dynamic Attention Reallocation</a></div> <div><a href="#TrueMICL">TrueMICL: A Dedicated MICL Dataset</a></div> <div><a href="#experiments">Experimental Results</a></div> </nav> </d-contents> <p>Multimodal Large Language Models (MLLMs), built on powerful language backbones, have enabled Multimodal In-Context Learning (MICL)â€”adapting to new tasks from a few multimodal demonstrations consisting of images, questions, and answers. Despite showing noticeable improvement on standard vision-language datasets, current MLLMs struggle to leverage visual information in the demonstrations. Specifically, they tend to neglect visual cues and over-rely on textual patterns, leading to mere text imitation rather than genuine multimodal adaptation. This behavior makes MICL still unimodal and largely restricts its practical utility. </p> <section id="introduction"> <h2>MICL and Visual Context Neglect</h2> <p> Multimodal In-Context Learning (MICL) enables models to adapt to new tasks from few multimodal demonstrations. However, current MLLMs struggle to effectively utilize visual information in demonstrations, tending to overlook visual cues and over-rely on textual patterns. This results in textual pattern imitation rather than genuine multimodal adaptation. <d-figure> <figure> <img src="/assets/img/micl-colm/fig1.jpg" alt="MICL Problem Illustration"> <figcaption>Examples of using MICL to solve image captioning from MSCOCO (top) and Clock Math from our proposed dataset, TrueMICL (bottom). Generating captions relies more on task recognition and can be answered based on text patterns in demos, without deep understanding of demo images. However, our task requires task learning where the model needs to learn the relationship between text and images in the demos. </figcaption> </figure> </d-figure> </p> <p> This limitation is often concealed by improved performance on tasks that do not require deep visual context understanding. For example, models can generate reasonable captions for query images even without referencing demo images, as they rely on textual pattern following rather than multimodal understanding. </p> </section> <section id="method"> <h2>DARA: Dynamic Attention Reallocation</h2> <p> To address the visual context neglect in MICL, we introduce <strong>Dynamic Attention Reallocation (DARA)</strong>, an efficient fine-tuning strategy that encourages models to attend to visual context by rebalancing attention across visual and textual tokens. DARA introduces a set of learnable attention-balancing parameters that dynamically regulate the influence of visual and textual tokens during attention computation. </p> <h3 id="efficiency">Lightweight and Efficient</h3> <p> DARA is remarkably lightweight, introducing only a small number of learnable parameters for rapid adaptation. We insert DARA into the first transformer layer of the language backbone, specifically targeting attention score matrices in all attention heads. Given 5 images (4-shot demos + query) and 32 attention heads, the total number of learnable parameters is only 5Ã—32=160. With around 100 parameters, DARA achieves up to 10% improvement on downstream tasks. </p> <h3 id="visualization">Visualization of DARA's Reallocation Effects</h3> <p> To better understand how DARA affects attention, we present both qualitative and quantitative visualizations showing DARA's impact on attention patterns. </p> <h4>Spatial Attention Analysis</h4> <p> The spatial attention heatmap shows that without DARA, both demonstration and query images receive minimal attention, as indicated by predominantly blue regions. After applying DARA, attention over image tokens increases markedly (more red/yellow areas), indicating enhanced attention to visual input and improved visual grounding. <d-figure> <figure> <img src="/assets/img/micl-colm/heatmap.png" alt="Attention Heatmaps" style="max-width: 600px; width: 100%; display: block; margin-left: auto; margin-right: auto;"> <figcaption>Attention heatmaps over input images without (top) and with (bottom) applying DARA. DARA enhances visual attention both qualitatively, showing increased focus on image regions.</figcaption> </figure> </d-figure> </p> <h4>Quantitative Attention Distribution</h4> <p> We quantitatively compare attention allocation across different modality tokens. Without DARA, the model allocates only 28% of attention to image tokens, focusing primarily on text. With DARA, this increases to 46.7%, demonstrating a substantial shift toward visual content during response generation. <d-figure> <figure> <img src="/assets/img/micl-colm/chart_clean.png" alt="Attention Distribution Chart" style="max-width: 500px; width: 100%; display: block; margin-left: auto; margin-right: auto;"> <figcaption>Normalized attention ratios over image and text tokens without and with applying DARA. DARA significantly increases attention allocation to visual content from 28% to 46.7%.</figcaption> </figure> </d-figure> </p> <h4>Head-wise Attention Amplification</h4> <p> The learned attention amplification factors across different attention heads and images reveal structured visual emphasis. DARA induces clear redistribution: demo images consistently receive factors larger than 1, encouraging stronger reliance on context, while different heads specialize in different aspects - for example, Head 1 emphasizes Demo 2 (1.27), while Head 5 emphasizes Demo 4 (1.32). <d-figure> <figure> <img src="/assets/img/micl-colm/heathead.png" alt="Head-wise Analysis" width="70%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption>Learned attention amplification factors across 8 heads and 5 images in the first transformer layer of Qwen2-VL. Values larger than 1 indicate attention amplification on visual tokens, showing DARA's selective, context-aware visual attention during MICL.</figcaption> </figure> </d-figure> </p> </section> <section id="TrueMICL"> <h2>TrueMICL: A Dedicated MICL Dataset</h2> <p> We introduce <strong>TrueMICL</strong>, a MICL-dedicated dataset designed with a critical principle: correct responses must rely on comprehensive understanding of multimodal context, especially visual information. Unlike existing MICL datasets that focus on task recognition, TrueMICL emphasizes task learning where models must understand relationships between visual and textual elements. </p> <h3 id="dataset-design">Dataset Design Principles</h3> <p> To this end, we have designed a novel MICL-dedicated dataset, TrueMICL, guided by the following principles: </p> <div style="margin: 20px 0;"> <ol style="line-height: 1.8; font-size: 16px;"> <li> <strong>Context Dependency:</strong> The task must be unsolvable without the context images. This ensures that models cannot rely solely on textual patterns or prior knowledge.</li> <li> <strong>Novelty:</strong> The task should introduce novel image-text relationships that are uncommon in pre-training or instruction tuning, to effectively challenge task learning ability.</li> <li> <strong>Perceivable Visual Information:</strong> The necessary information extracted from the images should not be overly complex, ensuring that the visual encoder can perceive it accurately. This allows us to focus on MICL ability rather than visual perception challenges.</li> <li> <strong>Compatibility with Backbone:</strong> The task should push the boundaries of multimodal in-context learning without exceeding the language backbone's capabilities.</li> <li> <strong>Configurability and Extensibility:</strong> This pipeline should be easily configured to generate more data samples with different levels of difficulty.</li> </ol> </div> <p> TrueMICL comprises 867 samples across 4 task types and 7 distinct tasks, covering mathematical reasoning, pattern finding, and novel visual concept learning. The dataset is designed to be scalable and configurable for different levels of difficulty. </p> <h3 id="dataset-examples">Dataset Examples</h3> <p> The table below shows representative examples from TrueMICL, illustrating how each task requires understanding the relationship between images and text in demonstrations to correctly answer queries. </p> <div style="overflow-x: auto; margin: 20px 0;"> <table style="width: 100%; border-collapse: collapse; font-size: 14px;"> <thead> <tr style="background-color: #f5f5f5;"> <th style="border: 1px solid #ddd; padding: 8px; text-align: center;">Category</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: center;">Task</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: center;" colspan="2">Demo 1</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: center;" colspan="2">Demo 2</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: center;">Query</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: center;">Label</th> <th style="border: 1px solid #ddd; padding: 8px; text-align: center;">Explanation</th> </tr> </thead> <tbody> <tr> <td rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: center; vertical-align: middle; background-color: #f9f9f9;">Math Reasoning</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Operator Induction</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/OI-d1.png" alt="OI Demo 1" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">0</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/OI-d2.png" alt="OI Demo 2" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">12</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/OI-q.png" alt="OI Query" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">8</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Multiplying the two numbers in the image</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Clock Math</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CM-d1.png" alt="CM Demo 1" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">14</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CM-d2.png" alt="CM Demo 2" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">3</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CM-q.png" alt="CM Query" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">20</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Adding the two numbers in the clock</td> </tr> <tr> <td rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: center; vertical-align: middle; background-color: #f9f9f9;">Concept Binding</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Outlier Detection</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/SC-d1.png" alt="SC Demo 1" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Green</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/SC-d2.png" alt="SC Demo 2" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Black</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/SC-q.png" alt="SC Query" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Red</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">The outlier color in the image</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">CLEVR Count</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CL-d1.png" alt="CL Demo 1" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">2</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CL-d2.png" alt="CL Demo 2" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">5</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CL-q.png" alt="CL Query" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">2</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">The number of spheres</td> </tr> <tr> <td rowspan="2" style="border: 1px solid #ddd; padding: 8px; text-align: center; vertical-align: middle; background-color: #f9f9f9;">Pattern Finding</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Sudoku</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/SD-d1.png" alt="SD Demo 1" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">918</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/SD-d2.png" alt="SD Demo 2" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">217</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/SD-q.png" alt="SD Query" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">470</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">The missing number in between</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Palindrome</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/PN-d1.png" alt="PN Demo 1" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">7</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/PN-d2.png" alt="PN Demo 2" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">9</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/PN-q.png" alt="PN Query" style="max-width: 80px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">9</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">To form palindrome number</td> </tr> <tr> <td style="border: 1px solid #ddd; padding: 8px; text-align: center; background-color: #f9f9f9;">Novel Concept</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Character Classification</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CC-d1.jpg" alt="CC Demo 1" style="max-width: 60px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Flora</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CC-d2.jpg" alt="CC Demo 2" style="max-width: 60px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Walter</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;"><img src="/assets/img/micl-colm/CC-q.jpg" alt="CC Query" style="max-width: 60px; height: auto;"></td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">Flora</td> <td style="border: 1px solid #ddd; padding: 8px; text-align: center;">The same character in the demo</td> </tr> </tbody> </table> </div> <p style="font-size: 14px; color: #666; margin-top: 10px;"> <strong>Table:</strong> An overview of task examples in TrueMICL. The label to the query requires the model to learn the relationship between images and text in the demonstrations. </p> </section> <section id="experiments"> <h2>Experimental Results</h2> <p> Our comprehensive experiments across various MLLMs demonstrate the effectiveness of both DARA and TrueMICL. Current MLLMs find TrueMICL evaluation tasks quite challenging, and DARA significantly improves MICL performance on both our evaluation tasks and standard VL tasks. <d-figure> <figure> <img src="/assets/img/micl-colm/results-table.png" alt="Experimental Results" width="90%" style="display: block; margin-left: auto; margin-right: auto;"> <figcaption>Comparison of different methods on TrueMICL dataset showing significant improvements with DARA fine-tuning across multiple model architectures.</figcaption> </figure> </d-figure> </p> <h3 id="results">Key Findings</h3> <p> </p> <ul> <li> <strong>Significant MICL Improvements:</strong> DARA achieves up to 10% improvement on downstream tasks with minimal parameter overhead</li> <li> <strong>Enhanced Visual Attention:</strong> Attention visualization confirms that DARA successfully redirects focus to visual content</li> <li> <strong>Challenging Evaluation:</strong> TrueMICL reveals limitations of current MLLMs in genuine multimodal understanding</li> <li> <strong>Broad Applicability:</strong> DARA shows consistent improvements across different model architectures and scales</li> </ul> </section> <section id="conclusion"> <h2>Takeaways</h2> <div style="background-color: #f8f9fa; border-left: 4px solid #03A6A1; padding: 20px; margin: 20px 0; border-radius: 5px;"> <h3 style="margin-top: 0; color: #03A6A1;">ðŸŽ¯ Main Contributions</h3> <ul style="line-height: 1.7;"> <li> <strong>Visual Context Neglect Problem:</strong> We identify and address a critical limitation in current MLLMs - their tendency to neglect visual information in multimodal demonstrations, leading to superficial text imitation rather than genuine multimodal learning.</li> <li> <strong>DARA Method:</strong> Our Dynamic Attention Reallocation approach provides an efficient solution with minimal parameters (as few as 160), achieving up to 10% performance improvements by strategically rebalancing attention toward visual content.</li> <li> <strong>TrueMICL Benchmark:</strong> We introduce a rigorous evaluation dataset specifically designed to test true multimodal in-context learning capabilities, revealing significant gaps in current model performance.</li> </ul> </div> <div style="background-color: #fff3cd; border-left: 4px solid #ffc107; padding: 20px; margin: 20px 0; border-radius: 5px;"> <h3 style="margin-top: 0; color: #856404;">ðŸ’¡ Key Insights</h3> <ul style="line-height: 1.7;"> <li> <strong>Attention Matters:</strong> Visualization analysis confirms that DARA successfully redirects model attention from text-dominant (28%) to more balanced multimodal processing (46.7% visual attention).</li> <li> <strong>Parameter Efficiency:</strong> DARA demonstrates that targeted architectural modifications can be more effective than large-scale parameter tuning, requiring 40-50Ã— fewer parameters than LoRA to achieve similar performance.</li> <li> <strong>Evaluation Gap:</strong> Standard VL benchmarks fail to capture true MICL capabilities, highlighting the need for dedicated evaluation frameworks like TrueMICL.</li> </ul> </div> <div style="background-color: #d1ecf1; border-left: 4px solid #17a2b8; padding: 20px; margin: 20px 0; border-radius: 5px;"> <h3 style="margin-top: 0; color: #0c5460;">ðŸš€ Future Implications</h3> <ul style="line-height: 1.7;"> <li> <strong>Practical Impact:</strong> DARA's lightweight design makes it easily deployable in resource-constrained environments while providing consistent improvements across different model architectures.</li> <li> <strong>Research Direction:</strong> Our work opens new avenues for investigating attention mechanisms in multimodal learning and designing more effective MICL evaluation protocols.</li> <li> <strong>Broader Applications:</strong> The principles behind DARA can potentially be extended to other multimodal tasks beyond in-context learning, wherever visual attention reallocation is beneficial.</li> </ul> </div> </section> </d-article> <d-appendix> <h3>BibTeX</h3> <p class="bibtex"> @misc{chen2024neglected,<br> Â Â title={From Neglected to Indispensable: Emphasizing the Visual Context for Multimodal In-Context Learning},<br> Â Â author={Shuo Chen and Jianzhe Liu and Zhen Han and Yan Xia and Daniel Cremers and Philip Torr and Volker Tresp and Jindong Gu}<br> Â Â year={2024},<br> Â Â conference={COLM 2025},<br> Â Â primaryClass={cs.CV}<br> } </p> <h3>Acknowledgement</h3> <p class="bibtex">This template is adapted from <a href="https://tsb0601.github.io/mmvp_blog/" rel="external nofollow noopener" target="_blank">Eyes Wide Shut</a>.</p> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </body> </html>